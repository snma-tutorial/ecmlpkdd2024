{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Mitigating biased node rankings\n",
    "## Overview\n",
    "In this exercise, we will continue to extend `netin` to create custom models.\n",
    "We will explore an extended use-case to study the effects of network growth interventions.\n",
    "While we focus on synthetic data for now, `netin` provides interfaces to load real networks and run the simulation on top of those networks.\n",
    "\n",
    "## Key Concepts\n",
    "**Custom Models**: Define and simulate custom models to test your own modelling ideas.\n",
    "In an extended use-case, we re-create a model to analyze how interventions in the growth process of the network can affect the visibility of the minority group.\n",
    "\n",
    "## Task\n",
    "1. Extend a custom class that considers two groups and homophilic interactions instead of just one. Add preferential attachment to the model mix.\n",
    "2. Create a custom model that is defined by a pre- and post-intervention phase. Through the intervention, the model parameters might change. Analyze and visualize how various parameter changes impact the visibility of the minority."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model extensions (continued)\n",
    "Various steps of analysis require modification of the existing models.\n",
    "For instance, you may want to create custom models that only slightly change the pre-defined simulation logic or retrieve additional analytics about the simulation process.\n",
    "The package is highly modular and provides interfaces for these use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### If running this on Google Colab, run the following lines:\n",
    "\n",
    "import os\n",
    "!pip install netin==2.0.0a1\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional, Tuple, List\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from netin.models import\\\n",
    "    PAHModel,\\\n",
    "    UndirectedModel\n",
    "from netin.graphs import\\\n",
    "    Graph,\\\n",
    "    BinaryClassNodeVector\n",
    "from netin.utils import MINORITY_LABEL, MAJORITY_LABEL, Event\n",
    "from netin.link_formation_mechanisms import\\\n",
    "    TwoClassHomophily, PreferentialAttachment\n",
    "import netin.utils.constants as const"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom models\n",
    "The code is structured to be extendable.\n",
    "You can create custom models by combining existing link formation mechanisms or by creating new ones.\n",
    "\n",
    "Let's create a model that has two group attributes and combines homophily based on a product of the homophily probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoGroupHomophilyModel(UndirectedModel):\n",
    "    SHORT = \"TGH\" # Short name of the model\n",
    "\n",
    "    def __init__(self, N:int, m:int,\n",
    "                 f_m:float,\n",
    "                 h0:float, h1:float,\n",
    "                seed:  Optional[Union[int, np.random.Generator]] = None):\n",
    "        \"\"\"Constructor for the TwoGroupHomophilyModel class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        N : int\n",
    "            Number of total nodes\n",
    "        m : int\n",
    "            Number of edges to attach from a new node to existing nodes\n",
    "        f_m : float\n",
    "            Fraction of minority nodes\n",
    "        h0 : float\n",
    "            Homophily for the first attribute\n",
    "        h1 : float\n",
    "            Homophily for the second attribute\n",
    "        seed : Union[int, np.random.Generator], optional\n",
    "            Random seed, by default 1\n",
    "        \"\"\"\n",
    "        super().__init__(N=N, m=m, seed=seed)\n",
    "        # Handle remaining parameters\n",
    "        self.f_m = f_m\n",
    "        self.h0 = h0\n",
    "        self.h1 = h1\n",
    "\n",
    "    def _initialize_node_classes(self):\n",
    "        \"\"\"Initializes two node classes for each node.\n",
    "        \"\"\"\n",
    "        self.graph.set_node_class(\n",
    "            \"attribute_0\", # Attribute name\n",
    "            # Values drawn randomly from the minority fraction\n",
    "            BinaryClassNodeVector.from_fraction(\n",
    "                N=self.N,\n",
    "                f_m=self.f_m,\n",
    "                class_labels=[MAJORITY_LABEL, MINORITY_LABEL],\n",
    "                rng=self._rng)\n",
    "        )\n",
    "        self.graph.set_node_class(\n",
    "            \"attribute_1\",\n",
    "            BinaryClassNodeVector.from_fraction(\n",
    "                N=self.N,\n",
    "                f_m=self.f_m,\n",
    "                class_labels=[MAJORITY_LABEL, MINORITY_LABEL],\n",
    "                rng=self._rng)\n",
    "        )\n",
    "\n",
    "    def _initialize_lfms(self):\n",
    "        \"\"\"Initializes the two homophily link formation mechanisms.\n",
    "        One for each attribute.\n",
    "        \"\"\"\n",
    "        self.lfm_h0 = TwoClassHomophily.from_two_class_homophily(\n",
    "            node_class_values=self.graph.get_node_class(\"attribute_0\"),\n",
    "            homophily=(self.h0, self.h0))\n",
    "        self.lfm_h1 = TwoClassHomophily.from_two_class_homophily(\n",
    "            node_class_values=self.graph.get_node_class(\"attribute_1\"),\n",
    "            homophily=(self.h1, self.h1))\n",
    "\n",
    "    def compute_target_probabilities(self, source: int) -> np.ndarray:\n",
    "        \"\"\"Compute the target probabilities for a given source node.\n",
    "        This function is being called in each simulation iteration to\n",
    "        compute the target probabilities for all potential target nodes.\n",
    "        Here, we simply multiply the homophily values based on the\n",
    "        two node attributes of the source node and target nodes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        source : int\n",
    "            Source node\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            Target probabilities for the source node\n",
    "        \"\"\"\n",
    "        # Calling super() applies the pre-defined filters to avoid\n",
    "        # self-loops and multiple edges\n",
    "        return super().compute_target_probabilities(source)\\\n",
    "            * self.lfm_h0.get_target_probabilities(source)\\\n",
    "            * self.lfm_h1.get_target_probabilities(source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because most of the simulation logic is contained in the parent class, we can run the simulation as we used to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "m = 3\n",
    "f_m = 0.3\n",
    "\n",
    "tgh_model = TwoGroupHomophilyModel(N=N, m=m, f_m=f_m, h0=0.5, h1=0.8, seed=123)\n",
    "tgh_graph = tgh_model.simulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the network, we export it to NetworkX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_tgh_model(graph: Graph):\n",
    "    tgh_nx_graph = graph.to_nxgraph()\n",
    "    pos = nx.kamada_kawai_layout(tgh_nx_graph)\n",
    "\n",
    "    colors = []\n",
    "    # Assign one out of four node colors based on the two attribute combinations\n",
    "    for _, data in tgh_nx_graph.nodes(data=True):\n",
    "        if data[\"attribute_0\"] == 0 and data[\"attribute_1\"] == 0:\n",
    "            colors.append(\"red\")\n",
    "        elif data[\"attribute_0\"] == 1 and data[\"attribute_1\"] == 1:\n",
    "            colors.append(\"green\")\n",
    "        elif data[\"attribute_0\"] == 0 and data[\"attribute_1\"] == 1:\n",
    "            colors.append(\"blue\")\n",
    "        elif data[\"attribute_0\"] == 1 and data[\"attribute_1\"] == 0:\n",
    "            colors.append(\"yellow\")\n",
    "\n",
    "    nx.draw_networkx_nodes(\n",
    "        tgh_nx_graph,\n",
    "        pos=pos,\n",
    "        node_color=colors,\n",
    "        node_size=[15*deg for deg in dict(tgh_nx_graph.degree()).values()])\n",
    "    nx.draw_networkx_edges(tgh_nx_graph, pos=pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_tgh_model(tgh_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks**:\n",
    "1. Extend the class to include preferential attachment as a link formation mechanism.\n",
    "Following the original models, the target probabilities to connect to a new node should be the product of the `Homophily` and `PreferentialAttachment` mechanisms.\n",
    "For the former, you can reuse `TwoGroupHomophilyModel.compute_target_probabilities()` and multiply it with `PreferentialAttachment.get_target_probabilities()`.\n",
    "   - `PreferentialAttachment` requires the graph when it is initialized to keep track of the degree changes. It is stored as a parent class attribute (accessible via `self.graph`).\n",
    "   - You can extend `TwoGroupHomophilyModel` and override only `_initialize_lfms` and `compute_target_probabilities`. Make sure to call `super().compute_target_probabilities()` to include the target probabilities computed by `TwoGroupHomophilyModel`.\n",
    "2. Plot the result using `draw_tgh_model()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PATGHModel(TwoGroupHomophilyModel):\n",
    "    SHORT = \"PATGH\"\n",
    "\n",
    "    def _initialize_lfms(self):\n",
    "        super()._initialize_lfms()\n",
    "        self.pa = PreferentialAttachment(N=self.N, graph=self.graph)\n",
    "\n",
    "    def compute_target_probabilities(self, source: int) -> np.ndarray:\n",
    "        return super().compute_target_probabilities(source) * self.pa.get_target_probabilities(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "patgh_model = PATGHModel(N=N, m=m, f_m=f_m, h0=0.5, h1=0.8, seed=123)\n",
    "patgh_graph = patgh_model.simulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_tgh_model(patgh_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case: testing network growth interventions\n",
    "As a showcase and advanced exercise, we use the `NetIn` package to study the effects of growth interventions introduced to the network.\n",
    "Based on a recent work by Neuhäuser et al. [[1]](#references), we try to understand how growing the minority group or changing the connection behavior can affect the visibility of the minority group.\n",
    "\n",
    "The model is defined by the two stages of pre- and post-intervention.\n",
    "Both stages implement a `PAHModel` that add `N // 2` nodes, with the second stage preloading the simulated graph of the first stage.\n",
    "Moreover, both models can have different parameters.\n",
    "We interpret the changes in the homophily parameter and minority fraction as as behavioral or group size intervention, respectively.\n",
    "Such a `GrowthInterventionModel` can be defined by the transition $PAH(f_{pre}, h_{pre}) \\rightarrow PAH(f_{post}, h_{post})$ with\n",
    "- minority fraction $f$\n",
    "- homophily parameter $h$\n",
    " \n",
    " **Task**:\n",
    "1. Create a custom `GrowthInterventionModel` by filling out the code snippet below, fulfilling the following requirements\n",
    " - It should inherit from `UndirectedModel` (this includes the initialization logic and event handling).\n",
    " - It should take as additional parameters: `h_pre`, `h_post`, `f_m_pre` and `f_m_post`.\n",
    " - Overwrite the `simulate()`-function to\n",
    "   - Create two `PAHModels`, parameterized by the pre- and post-intervention `h` and `f_m`.\n",
    "   - Simulate the first model and trigger the predefined `EVENT_GROWTH_INTERVENTION` event.\n",
    "   - Preload the created `Graph` for the second model and return its simulation result (_Hint_: Use `Model.preload_graph`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENT_GROWTH_INTERVENTION = \"GROWTH_INTERVENTION\"\n",
    "\n",
    "class GrowthInterventionModel(UndirectedModel):\n",
    "    SHORT = \"GI\" # Short name of the model\n",
    "\n",
    "    # Add the custom event to the list of events triggered by this class\n",
    "    EVENTS = UndirectedModel.EVENTS + [EVENT_GROWTH_INTERVENTION]\n",
    "\n",
    "    h_pre: float\n",
    "    h_post: float\n",
    "\n",
    "    f_m_pre: float\n",
    "    f_m_post: float\n",
    "\n",
    "    def __init__(\n",
    "            self, *args,\n",
    "            N: int,\n",
    "            m: int,\n",
    "            f_m_pre: float, f_m_post: float,\n",
    "            h_pre: float, h_post: float,\n",
    "            seed:  Optional[Union[int, np.random.Generator]] = None,\n",
    "            **kwargs):\n",
    "        # Set the parameters and forward the remaining arguments to the super class\n",
    "        ################\n",
    "        ## YOUR CODE HERE\n",
    "        ################\n",
    "        pass\n",
    "\n",
    "    def simulate(self) -> Graph:\n",
    "        # Create the first half of the graph by simulating a ``PAHModel``.\n",
    "        model_pre = PAHModel(\n",
    "            ################\n",
    "            ## YOUR CODE HERE\n",
    "            ################\n",
    "        )\n",
    "        graph_pre = model_pre.simulate()\n",
    "\n",
    "        # Remove the default event handler for adding links\n",
    "        graph_pre.remove_event_handler(Event.LINK_ADD_AFTER)\n",
    "\n",
    "        # Trigger the custom event to allow for modifications\n",
    "        ################\n",
    "        ## YOUR CODE HERE\n",
    "        ################\n",
    "\n",
    "        # Create the second half of the graph by simulating another ``PAHModel``.\n",
    "        ################\n",
    "        ## YOUR CODE HERE\n",
    "        ################\n",
    "        model_post = PAHModel(\n",
    "            ################\n",
    "            ## YOUR CODE HERE\n",
    "            ################\n",
    "        )\n",
    "\n",
    "        # Preload the graph from the first half and return the simulation result\n",
    "        ################\n",
    "        ## YOUR CODE HERE\n",
    "        ################\n",
    "\n",
    "        return model_post.simulate()\n",
    "\n",
    "    def _initialize_lfms(self):\n",
    "        # Needed to avoid errors when calling the super class\n",
    "        pass\n",
    "\n",
    "    def _initialize_node_classes(self):\n",
    "        # Needed to avoid errors when calling the super class\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENT_GROWTH_INTERVENTION = \"GROWTH_INTERVENTION\"\n",
    "\n",
    "class GrowthInterventionModel(UndirectedModel):\n",
    "    SHORT = \"GI\" # Short name of the model\n",
    "\n",
    "    # Add the custom event to the list of events triggered by this class\n",
    "    EVENTS = UndirectedModel.EVENTS + [EVENT_GROWTH_INTERVENTION]\n",
    "\n",
    "    h_pre: float\n",
    "    h_post: float\n",
    "\n",
    "    f_m_pre: float\n",
    "    f_m_post: float\n",
    "\n",
    "    def __init__(\n",
    "            self, *args,\n",
    "            N: int,\n",
    "            m: int,\n",
    "            f_m_pre: float, f_m_post: float,\n",
    "            h_pre: float, h_post: float,\n",
    "            seed:  Optional[Union[int, np.random.Generator]] = None,\n",
    "            **kwargs):\n",
    "        # Set the parameters and forward the remaining arguments to the super class\n",
    "        super().__init__(\n",
    "            *args, N=N, m=m, seed=seed, **kwargs)\n",
    "        self.f_m_pre = f_m_pre\n",
    "        self.f_m_post = f_m_post\n",
    "        self.h_pre = h_pre\n",
    "        self.h_post = h_post\n",
    "\n",
    "    def simulate(self) -> Graph:\n",
    "        # Create the first half of the graph by simulating a ``PAHModel``.\n",
    "        model_pre = PAHModel(\n",
    "            N=self.N // 2,\n",
    "            f_m=self.f_m_pre,\n",
    "            m=self.m,\n",
    "            h_m=self.h_pre,\n",
    "            h_M=self.h_pre,\n",
    "            seed=self._rng)\n",
    "        graph_pre = model_pre.simulate()\n",
    "\n",
    "        # Remove the default event handler for adding links\n",
    "        graph_pre.remove_event_handler(Event.LINK_ADD_AFTER)\n",
    "\n",
    "        # Trigger the custom event to allow for modifications\n",
    "        self.trigger_event(event=EVENT_GROWTH_INTERVENTION, graph=graph_pre)\n",
    "\n",
    "        # Create the second half of the graph by simulating another ``PAHModel``.\n",
    "        model_post = PAHModel(\n",
    "            N=self.N // 2,\n",
    "            f_m=self.f_m_post,\n",
    "            m=self.m,\n",
    "            h_m=self.h_post,\n",
    "            h_M=self.h_post,\n",
    "            seed=self._rng)\n",
    "\n",
    "        # Preload the graph from the first half and return the simulation result\n",
    "        model_post.preload_graph(graph=graph_pre)\n",
    "        return model_post.simulate()\n",
    "\n",
    "    def _initialize_lfms(self):\n",
    "        # Needed to avoid errors when calling the super class\n",
    "        pass\n",
    "\n",
    "    def _initialize_node_classes(self):\n",
    "        # Needed to avoid errors when calling the super class\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the original article, we assess the impact of growth interventions by the representation of minority nodes among the highest degree nodes before and after the intervention.\n",
    "\n",
    "**Tasks**\n",
    "1. Write a function `get_top_p_minority_fraction` that takes a `Graph` and a percentile `top_p` parameter and returns the fraction of minority nodes among the nodes with a degree that is higher than the `top_p` percentile border.\n",
    "   - _Hint_: Use `numpy.percentile`, `Graph.degrees()` and `Graph.get_node_class(const.CLASS_ATTRIBUTE)`. Recall that `NodeVector` can be used like a numpy array.\n",
    "2. Report the impact of the following two interventions $PAH(f_{pre}, h_{pre}) \\rightarrow PAH(f_{post}, h_{post})$ by printing the result of `get_top_p_minority_fraction` before and after the intervention:\n",
    "   1. Behavioral: $f_{pre} = f_{post} = 0.1$, $h_{pre}=0.2$, $h_{post} = 0.8$\n",
    "   2. Group size: $f_{pre} = 0.1, f_{post} = 0.5$, $h_{pre} = h_{post} = 0.8$\n",
    "   - _Hint_: Utilize `Model.register_event_handler` to make use of the `EVENT_GROWTH_INTERVENTION` to compute the pre-intervention fraction.\n",
    "   - _Bonus points_: Run the simulation a couple of times and report the averages to account for random fluctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_P = 0.1\n",
    "\n",
    "N = 1000\n",
    "m = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_p_minority_fraction(\n",
    "        graph: Graph,\n",
    "        top_p: float = TOP_P) -> float:\n",
    "    \"\"\"Gets the minority fraction of nodes with a degree above the top p degree percentile.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph : Graph\n",
    "        The graph to analyze.\n",
    "    top_p : float\n",
    "        The top p percentile.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The minority fraction of nodes with a degree above the top p percentile.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the node classes and degrees\n",
    "    minority_nodes = graph.get_node_class(\n",
    "        const.CLASS_ATTRIBUTE)\n",
    "    degrees = graph.degrees()\n",
    "\n",
    "    # Get the degree cutoff based on the top p percentile\n",
    "    d_cutoff = np.quantile(degrees, 1 - top_p)\n",
    "\n",
    "    return np.mean(minority_nodes[degrees >= d_cutoff])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(f_m_pre: float, f_m_post: float, h_pre: float, h_post: float, n_iter: int = 10)\\\n",
    "    -> Tuple[List[float], List[float]]:\n",
    "    # Keep track of the pre- and post-intervention top p minority fractions\n",
    "    # for each simulation run\n",
    "    f_m_top_pre = []\n",
    "    f_m_top_post = []\n",
    "\n",
    "    # Define the model\n",
    "    model = GrowthInterventionModel(\n",
    "        N=N, m=m,\n",
    "        f_m_pre=f_m_pre, f_m_post=f_m_post,\n",
    "        h_pre=h_pre, h_post=h_post\n",
    "    )\n",
    "\n",
    "    # Define a function to keep track of the top p minority fraction\n",
    "    # prior to the intervention\n",
    "    def add_intervention_top_p(graph: Graph, f_m_top_pre=f_m_top_pre):\n",
    "        f_m_top_pre.append(\n",
    "            get_top_p_minority_fraction(graph=graph))\n",
    "\n",
    "    # Register the function as an event handler\n",
    "    model.register_event_handler(\n",
    "        event=EVENT_GROWTH_INTERVENTION,\n",
    "        function=lambda graph: add_intervention_top_p(graph=graph))\n",
    "\n",
    "    # Run the simulation ten times\n",
    "    for _ in range(n_iter):\n",
    "        model._initialize_graph()\n",
    "        graph = model.simulate()\n",
    "\n",
    "        # Store the post-intervention minority fraction\n",
    "        f_m_top_post.append(\n",
    "            get_top_p_minority_fraction(graph=graph))\n",
    "\n",
    "    print((f\"Minority fraction in top {TOP_P:.0%} degree nodes for \"\n",
    "           f\"f_m_pre={f_m_pre}, f_m_post={f_m_post} and h_pre={h_pre}, h_post={h_post}:\"))\n",
    "    print(f\"\\tAfter first phase ({len(f_m_top_pre)} iterations): {np.mean(f_m_top_pre), np.std(f_m_top_pre)}\")\n",
    "    print(f\"\\tAfter second phase ({len(f_m_top_post)} iterations): {np.mean(f_m_top_post), np.std(f_m_top_post)}\")\n",
    "\n",
    "    return f_m_top_pre, f_m_top_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_m = 0.1\n",
    "h_pre, h_post = 0.2, 0.8\n",
    "f_m_top_pre, f_m_top_post = run_model(f_m_pre=f_m, f_m_post=f_m, h_pre=h_pre, h_post=h_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_m_pre, f_m_post = 0.1, 0.5\n",
    "h = 0.8\n",
    "f_m_top_pre, f_m_top_post = run_model(f_m_pre=f_m_pre, f_m_post=f_m_post, h_pre=h, h_post=h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial parameterization and intervention seem to have a big effect on the outcome.\n",
    "Let's visualize these effects more systematically.\n",
    "\n",
    "**Tasks**:\n",
    "1. Repeat the experiment for all combinations of $h_{pre}, h_{post} \\in \\{0.2, 0.5, 0.8\\}$, storing the pre- and post-intervention fraction of minority nodes among the top $10\\%$ degree percentile. Use at least ten iterations per configuration.\n",
    "2. Plot the results. Focus on the difference of the minority fraction in top ranks between pre- and post-intervention. You're free to choose your preferred form of visualization. For inspiration, you can follow Fig. 2a of [[1]](#references), displaying the line color based on $h_{pre}$, $h_{post}$ on the x-axis and the computed minority fractions on the y-axis differentiating between pre- and post-intervention by markers.\n",
    "   - _Hint_: You can use `pandas.DataFrame` and `matplotlib.errorbar` to store and visualize the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions\n",
    "1. Computing the impact of interventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_VALS = (0.2, 0.5, 0.8)\n",
    "f_m = 0.1\n",
    "\n",
    "df_data = defaultdict(list)\n",
    "\n",
    "def add_intervention_top_kp(graph: Graph):\n",
    "    global df_data\n",
    "    f_top = get_top_p_minority_fraction(graph=graph)\n",
    "    df_data[\"f_top_pre\"].append(f_top)\n",
    "\n",
    "for h_pre, h_post in product(H_VALS, repeat=2):\n",
    "    for _ in range(20):\n",
    "        model = GrowthInterventionModel(\n",
    "            N=N, m=m,\n",
    "            f_m_pre=f_m, f_m_post=f_m,\n",
    "            h_pre=h_pre, h_post=h_post\n",
    "        )\n",
    "        model.register_event_handler(\n",
    "            event=EVENT_GROWTH_INTERVENTION, function=add_intervention_top_kp)\n",
    "        graph = model.simulate()\n",
    "        f_top = get_top_p_minority_fraction(graph=graph)\n",
    "\n",
    "        df_data[\"h_pre\"].append(h_pre)\n",
    "        df_data[\"h_post\"].append(h_post)\n",
    "        df_data[\"f_top_post\"].append(f_top)\n",
    "df_data = pd.DataFrame(df_data)\n",
    "df_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data\\\n",
    "    .groupby([\"h_pre\", \"h_post\"])\\\n",
    "    .agg([\"mean\", \"std\"])\\\n",
    "    .sort_index()\n",
    "df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Plotting the impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = plt.Normalize(vmin=min(H_VALS), vmax=max(H_VALS))\n",
    "cmap = plt.colormaps[\"coolwarm\"]\n",
    "\n",
    "plt.axhline(y=f_m_pre, color=\"black\", linestyle=\"dashed\")\n",
    "\n",
    "for x_off, h_pre in zip([-.05, 0, .05], H_VALS):\n",
    "    df_data_h = df_data.loc[h_pre]\n",
    "    color = cmap(norm(h_pre)) if h_pre != H_VALS[1] else \"black\"\n",
    "\n",
    "    plt.axvline(x=h_pre, color=\"lightgray\", linestyle=\"dotted\")\n",
    "\n",
    "    plt.errorbar(\n",
    "        df_data_h.index + x_off, # h_post\n",
    "        df_data_h[(\"f_top_pre\", \"mean\")],\n",
    "        yerr=df_data_h[(\"f_top_pre\", \"std\")],\n",
    "        fmt=\"x\",\n",
    "        label=f\"$f_{{pre}}$, $h_{{pre}}={h_pre}$\",\n",
    "        color=color,\n",
    "        linestyle=\"dotted\",\n",
    "        capsize=5\n",
    "    )\n",
    "\n",
    "    plt.errorbar(\n",
    "        df_data_h.index + x_off, # h_post\n",
    "        df_data_h[(\"f_top_post\", \"mean\")],\n",
    "        yerr=df_data_h[(\"f_top_post\", \"std\")],\n",
    "        fmt=\"o\",\n",
    "        label=f\"$f_{{post}}$, $h_{{pre}}={h_pre}$\",\n",
    "        color=color,\n",
    "        linestyle=\"solid\",\n",
    "        capsize=5\n",
    "    )\n",
    "\n",
    "legend_elements = [\n",
    "    plt.Line2D(\n",
    "        [0], [0],\n",
    "        marker='x', color='black', markerfacecolor='black',\n",
    "        markersize=10,\n",
    "        label='Pre-intervention',\n",
    "        linestyle='dotted'),\n",
    "    plt.Line2D(\n",
    "        [0], [0],\n",
    "        marker='o', color='black', markerfacecolor='black',\n",
    "        markersize=10,\n",
    "        label='Post-intervention',\n",
    "        linestyle=\"solid\"),\n",
    "    plt.Line2D([0], [0], color=cmap(norm(H_VALS[0])), lw=2, label=f'$h_{{pre}}={H_VALS[0]}$'),\n",
    "    plt.Line2D([0], [0], color=\"black\", lw=2, label=f'$h_{{pre}}={H_VALS[1]}$'),\n",
    "    plt.Line2D([0], [0], color=cmap(norm(H_VALS[2])), lw=2, label=f'$h_{{pre}}={H_VALS[2]}$')\n",
    "]\n",
    "plt.legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(H_VALS)\n",
    "plt.xlabel(\"Post-intervention homophily $h_{{post}}$\")\n",
    "plt.ylabel(\"Minority fraction in top 10% degree nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Neuhäuser, L., Karimi, F., Bachmann, J., Strohmaier, M. & Schaub, M. T. Improving the visibility of minorities through network growth interventions. [Commun Phys 6, 1–13 (2023)](https://www.nature.com/articles/s42005-023-01218-9).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "netin-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
